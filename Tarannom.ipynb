{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uikg4VycMFIK",
        "outputId": "325115a9-1c58-4c71-ac09-b77cd1d68bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Datasets/my_poems.zip\n",
            "  inflating: my_poems - Copy.pkl     \n"
          ]
        }
      ],
      "source": [
        "!unzip \"/content/drive/MyDrive/Datasets/my_poems.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DZyfOKlLxGD",
        "outputId": "481e83ba-61f2-49cf-ffc6-23cbb9c423e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['خَليلَيَّ لا تَستَعجِلا أَن تَزَوَّدا وَأَن تَجمَعا شَملي وَتَنتَظِرا غَدا', 'فَما لَبَثٌ يَوماً بِسابِقٍ مَغنَمٍ وَلا سُرعَتي يَوماً بِسابِقَةِ الرَدى', 'وَإِن تُنظِراني اليَومَ أَقضِ لُبانَةً وَتَستَوجِبا مَنّاً عَلَيَّ وَتُحمَدا', 'لَعَمرُكَ ما نَفسٌ بِجِدٍ رَشيدَةٍ تُؤامِرُني سِرّاً لِأَصرِمَ مَرثَدا', 'وَإِن ظَهَرَت مِنهُ قَوارِصُ جَمَّةٌ وَأَفرَعَ في لَومي مِراراً وَأَصعَدا', 'عَلى غَيرِ ذَنبٍ أَن أَكونَ جَنَيتُهُ سِوى قَولِ باغٍ كادَني فَتَجَهَّدا', 'لَعَمري لَنِعمَ المَرءُ تَدعو بِحَبلِهِ إِذا ما المُنادي في المَقامَةِ نَدَّدا', 'عَظيمُ رَمادِ القِدرِ لا مُتَعَبِّسٌ وَلا مُؤيِسٌ مِنها إِذا هُوَ أَوقَدا', 'وَإِن صَرَّحَت كَحلٌ وَهَبَّت عَرِيَّةٌ مِنَ الريحِ لَم تَترُك لِذي المالِ مِرفَدا', 'صَبَرتُ عَلى وَطءِ المَوالي وَحَطمِهِم إِذا ضَنَّ ذو القُربى عَلَيهِم وَأَخمَدا']\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Read the list from the file\n",
        "with open(\"/content/my_poems - Copy.pkl\", \"rb\") as f:\n",
        "    my_list_loaded = pickle.load(f)\n",
        "\n",
        "# Print the loaded list\n",
        "print(my_list_loaded[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XhsuYO-CMSch"
      },
      "outputs": [],
      "source": [
        "full_text = \"\\n\".join(my_list_loaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HFGMAt6MiB4",
        "outputId": "7015f0a4-3b16-43e9-999e-ef78d2d42cb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "خَليلَيَّ لا تَستَعجِلا أَن تَزَوَّدا وَأَن تَجمَعا شَملي وَتَنتَظِرا غَدا\n",
            "فَما لَبَثٌ يَوماً بِسابِقٍ مَغنَمٍ وَلا سُرعَتي يَوماً بِسابِقَةِ الرَدى\n",
            "وَإِن تُنظِراني اليَومَ أَقضِ لُبانَةً وَتَستَوجِبا مَنّاً عَلَيَّ وَتُحمَدا\n",
            "لَعَمرُكَ ما نَفسٌ بِجِدٍ رَشيدَةٍ تُؤامِرُني سِرّاً لِأَصرِمَ مَرثَدا\n",
            "وَإِن ظَهَرَت مِنهُ قَوارِصُ جَمَّةٌ وَأَفرَعَ في لَومي مِراراً وَأَصعَدا\n",
            "عَلى غَيرِ ذَنبٍ أَن أَكونَ جَنَيتُهُ سِوى قَولِ باغٍ كادَني فَتَجَهَّدا\n",
            "لَعَمري لَنِعمَ المَرءُ تَدعو بِحَبلِهِ إِذا ما المُنادي في المَقامَةِ نَدَّدا\n",
            "عَظيمُ رَمادِ القِدرِ لا مُتَعَبِّسٌ وَلا مُؤيِسٌ مِنها إِذا هُوَ أَوقَدا\n",
            "وَإِن صَرَّحَت كَحلٌ وَهَبَّت عَرِيَّةٌ مِنَ الريحِ لَم تَترُك لِذي المالِ مِرفَدا\n",
            "صَبَرتُ عَلى وَطءِ المَوالي وَحَطمِهِم إِذا ضَنَّ ذو القُربى عَلَيهِم وَأَخمَدا\n",
            "وَلم يَحمِ فَرجَ الحَيِّ إِلّا مُحافِظٌ كَريمُ المُحَيّا ماجِدٌ غَيرُ أَحرَدا\n",
            "أَرى جارَتي خَفَّت وَخَفَّ نَصيحُها وَحُبَّ بِها لَولا النَوى وَطُموحُها\n",
            "فَبيني على نَجمٍ شَخيسٍ نُحوسُهُ وَأَشأَمُ طَيرِ الزاجِرينَ سَنيحُها\n",
            "فَإِن تَشغَبي فَالشَ\n",
            "103265586\n"
          ]
        }
      ],
      "source": [
        "print(full_text[0:1000])\n",
        "print(len(full_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hpa3QIuiT7ax"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 128 # how many independent sequences will we process in parallel?\n",
        "block_size = 64 # what is the maximum context length for predictions?\n",
        "max_iters = 10000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 1000\n",
        "n_embd = 128\n",
        "n_head = 8\n",
        "n_layer = 12\n",
        "dropout = 0.3\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "text = full_text\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "qcQHnHeZStUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2PkohZHcNAGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7f1597f-baab-450a-c7ce-5b59b6c8fca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "فَلن تنفُّمُ في عُداةٍ يا غُراريَ مِنْهُم أن تَشفي الحسب إنَّما يَروعُ\n",
            "جَازَ بها الرجومُ عليه مُفْتَخَلاً لأرض والفَضْلُ\n",
            "خلساً جمرُه أَلتتَها من حلَّاكَ ذاتٌ متسلِيل\n",
            "إن ما نرتُ علمائمُ انتدصَّ عصرُك هاماً لا أذمنَ جديماً وتحرُّرا\n",
            "مقامِ ذَمْنا وقَدْ ذا غانٍ ثهابكِ والهدى الزَّمَنا\n",
            "وأبو عيوْث وأنقناهُ لن تلومينا سَواؤك بالنبين فما لمن لم\n",
            "ترَبُّغ العيونُ وساً سِواك بها وساحب لفسرى أسوداً من جيبِه صبح\n",
            "يا وانظر سُمولُها ولا تضهم مَقامُيا في كلى حُلومِه\n",
            "أقتل فتلكه خُلةٌ أجمعني وإن كُنتُ أصلونا عن ذا حينا الغوض غور\n",
            "ولولا ظلم الدنا يا عندك يا أقمر آبائه والجواب السترج\n",
            "إذا ما استغتثرا لقنا عن عدا والخود لج الليالي الونيبُ\n",
            "أماع هذا العمر في قوم حزنة وشهبَ كل ليلًٍ كاس منه أمراً واعتَنا\n",
            "فلله بابِ الهوان إلى نَبو\n",
            "ولما نابَر فقد عَجُفَ النوى غواديثاً ولا خصماً حَسكراً طهولا\n",
            "ظلت من جودِهما حرسةُ عزى يكونُ من البمادِ ظُعْماً طَنينة مثلُ في كلِ\n",
            "تُودُّ لماذا مَلأُ سِوى قرمر عِزَّةٍ يُجدِّسنَهُ أرواحَهُ الرشدُ\n",
            "قالَ الشمسُ عنهُ معطرٌ وسكيرٌ شُكاٌ ولهُ الغلامةُ جنِبُ\n",
            "وأنصعَ والحافلونَ مَن ألوي في أظهرُ الدّاني عِمى فخانا\n",
            "يَا الأكبادُ طولَ الآنَ وكفُّها ولا بَرَد الذي عائدهةَ عثر الفؤا المبيد\n",
            "وبينِ الدهرنِ وجانبٍ ليس رمل الفمكةِ مسبِدَ وجهها\n",
            "اللَيلُ تخشى جاب كانت فيه محمودٍ صحيح نسيمهمْ التُّقَى الجراح\n",
            "تأمَّتْ لأمره ما دعا قلوب أباها من تلكهم في البرق منصر\n",
            "رسمي ومن يرىً وكفهم من حولهم فخرهم الخمر ما الجبالا\n",
            "عجز ما لاسوا إذا ما دمتم لوناً تنار بها\n",
            "هو الشعار من الجامع ليس بها الكجال على كامل الله عشيّةُ المثل\n",
            "يفاد بها دلجةً فيا حظِّ موفِديً فها الظَّلامُ ضحتْ طوالهُ\n",
            "أكثرهَا فاجدها بنُورِها أخشى خطباً كُمُ ما بَقَى كما من مليكٍ بقُطهُ المكا\n",
            "إذا البلا بصدق عن البدر فقد بعثُ حين خبَّهُ الدبد في الهوى وعابقُ\n",
            "فبَني الله من هَوايى للرأي زَجَى مع راووسِه صِبا نَاءه\n",
            "جَلٌّ لماكنّ ما جاءَتكَ يحسِبُ عَلَى الصدقِ على شُماع ولا وَلاتِقُ\n",
            "إِذا فاعَدتَ ذكَتكَ مَجهولُ ميَاضٍ بهم اِكفعُ حُشْداً فكرا\n",
            "فِداءً لطبعِكَ طهورٍ كلاَّ من ظمىءٍ أخذاً للقيان في كلِّ يومِ والصلحُ\n",
            "أمَا تضيج بسروي به وقارئط الحب يضحي لمن لم يدن الحرفُ وخلدُ\n",
            "فهذَا عبثاً ثارٍ لجت حولهِم جنبة الورْدِ والصفح جاهدةُ السّعيدِ\n",
            "ألن الفخرُ في زنة الرّاوية مإنسابةٍ وفي النّعامِ البدرِ\n",
            "وعجبٌ حللتُ وعى لَبنا ولا مغورُ وداءُ مَوتِ سواهمِ العِقِد\n",
            "فلا تدفي بوجهٍ فمسٍ نافلةٍ منيتٍ تاجلُّ ورامِ الوفق\n",
            "أو كلُّ ملبُولةٍ مَلامنْي كالشّهم\n",
            "نزمُ آفةً خَيلهما فالمرائه من أنِّ الشموس البتاجر لا تُعُدَّ الإذهَال\n",
            "وَقفلهم ألف ديفان اللجم منهم قلبُ وفي البرّ خالك\n",
            "تعمّوا من هداك لعبٍّ ولم يكن حراته منهم حادَّرت لهمْ رويْناً ولا كابد\n",
            "همّي للنبيِّتِ النصر علْواً فقد عُدَّ رغامُ خَضْراً في رواجِعِ مِجداً لابسا\n",
            "يا لمّا أشركَ المكانِ في الصدى وإنما شَربي القلبُ منّي انجمعَ الاخرَامْ\n",
            "فأضلَّ جميعَكُمُ في لجدٍ ببُهل قَلامِ اوصى الَّذي روحُكُمْ\n",
            "أما لم تكُلَّفوا القُلوبَ ليستَ مِن رغبٍ ما أنتَ سويدُهما أو آلاءُ\n",
            "وبالغفل وَانَّ بابِهِ وَأصبحَ هذا ما بِهِمل ما تَصِيلنها الأدْجُس\n",
            "وَلئنصرَةً قالَ وَخيرَ الفَضْلِ والفَحْجُ ضَمّ الفتى متجدِّداً منصر\n",
            "ما تَرَى الوَلَى لمِثلآ إلا هُ ولا المَذَاهدُ غدتْني للآلام لكن جائمُه\n",
            "كم إلا بأَنَّ الآجامَ مَدحَكَ إنما أَن هذي في الجوى وعادِ\n",
            "وقد قد رادَهُ الهِبا الاسم قد كان بهاو اللُّهبُ لهِ تضيقُ\n",
            "يا ابن كلّ علمة دمعهُ صبورةٌ وأ\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "# step 9999: train loss 1.8492, val loss 2.0209\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=3000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OnZt7VccOjcS"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/Datasets/poems_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div dir=\"rtl\">\n",
        "<h1>مختارات</h1>\n",
        "<p> أم ذكرتِ قبلاً فكان رشداً ولنا ما براه السرور الدما</p>\n",
        " </div>"
      ],
      "metadata": {
        "id": "vn63SftPBAW9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnEmcBImvDPb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TO8LI8VGMpte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jWNi8fZxO1f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc2b27b-ff5e-48df-8381-eaf16c0a2138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "سحرة أيلقيها الله صحوره بِالخطب للظلوم\n",
            "وتسهنة ما تشغل في الله فاجعل صسروها في أليالهم لمه الجملات\n",
            "في جُبنه لمَن ينظر عهدةً وافعي الهاتى لن يحوج\n",
            "وربّه هذا سير الإِضلال وللسلان أتمن لَ كل شوعٍ نحو خال\n",
            "فاشتَرني بروحه إذا لك ثارَتيْه فاشربكَ مُغْسِراً مِن نور أنوار\n",
            "فتمختْره بالسِّلمِ نوّارُ التجفي يمرلُ\n",
            "قد جاءني لحَدٌ لَهِ الجزال إِن جلت فجلَّ فيها الأسور\n",
            "ما قطعت اللِّومَ عنها لعفُه ضلالةً فأسلتها بنوحِ المَشكَّ في الأسحابُ\n",
            "انَّني لأمسى أرى لَها وأداكي على مَلكامِها والحيْ\n",
            "هاتها بفداق الوجود حرةً كنتُ ورقحونَ اللّهيب\n",
            "في باعيهِ شتف المنايا رجائِك\n",
            "هُو العلق لوج اللأحياءُ الزائرونَةُ في صَبرٍ آقل والعذِلِيِّ\n",
            "وسريلِ كامنٍ في هواها ونزليمٍ فوقَ الوثَم\n",
            "ويومٍ لمجدٍ نحتهُ سفينا كذا سحر نحافٍ ومصر يطير ليله\n",
            "عاسي حد شنتهِ الحياة فأنتَ لي ظهرها سواجدها\n",
            "وإذا امترد جطعتها كلَّ عربي بغصل من بها شرع قبلُها\n",
            "جرحت لغت وُقبي طربك مفرّحُ فيها وخيره شيغاً لا سامَ الولث\n",
            "ونامَت لها شواه فليس موليهِ مٌ ولا شلّت له فالعاملُ لا بُقْي\n",
            "أيقاسُ يخْلق إلى أثوابهِ ولا ألّف ضَمْضَةٌ عليهِ السّحراء\n",
            "لها إنْ يُزهدَ مِن كلِّ غِمرةِ العَصْرِ قَاتماً ولا فعرِي\n",
            "غروبَةٍ خَطَّةِ إسماعٍ أولايَ تُرِّيَةٌ لاقيتَ مثلُ تُلقَي لنا بِصبِّ ذيبُ\n",
            "شاكرُهُما أخفَي طُيبٌ أفيل من شَحَنٍ يُمشي الغواثُ مُمطارٌ فيا عَذبا\n",
            "لا نحنُ غِبطَ سبٍّ نَظَرتَ فَزيتهُما لَم أكُن إن توجدوكَ مَوجودَ الجَهلِ\n",
            "لِقائِبِكَ لَو أَنظُر لَفاغِمي أَوائِمُ بَيضاً دَفيَةُ مُختَرِجا\n",
            "تَجابُ لهُ رماتي أبداً تَعنو أَبوضاً أشْمباً هنَّمَ كِها اللهان يُخفِي\n",
            "وكأنّ تطدرُ لجُسُ عبدِهم ما تسافلُ كُلُّ أكل جوزيعٍ وحيدْ\n",
            "عمو يد الظباء حرمديهم شغل تعمي لَعد خرجة في الطبع النسيّد\n",
            "يَصارح ذلك الأرض مُخلِباً في عدلتها وجاء ما وَجَدتُ مثلك يابج من الأَفرّ\n",
            "أقولها فلا زَلَلتُ فتىً للعزم أن أَن تُمدي بالغر بعزمٍ تَذَوَّلُ\n",
            "وكفى الماء حُصْنُ النفس عليك\n",
            "كفائنا وتنورُ مرىً لأنت لهمْ بحرُ جفو ما آكاله ما سربوا\n",
            "ما ذا سَبيل الله أن وصلوهم إلى دنُّوا ببعضك في ذاك إن يرلغوا\n",
            "وَليّ للحق عذبوا أَضباب بي نوراً فاخْتَشبتْ نسيم قموم\n",
            "هياتُ المصاحة ترزقُ في همومه والهوى فصاراها وأقعَادها إما\n",
            "فدي عليهم صديقةُ من التجالما أسأق رازقهم اللقاء\n",
            "فمن قيل يزمت الريان إملاك الصديق سماها يباركها فتحت أركان\n",
            "دنا سنا أرصاد تاج ألت حرب الفتى وفي تراب بحر زدور\n",
            "وحيث واللَه قد مالكفه بالمحل والشائق\n",
            "وهو عمل باحي الجانح واليوم الهدى عنا تبعض موجَده\n",
            "والامين الحمام مذبج بتعريضة خاملة ذكره أكثق الزحل البلاد\n",
            "والأصل يروع علماً منك امام بثمام قد خل ولا العالمون قلما\n",
            "العدا في سلا نشر لهو حرباً للقوم عنه طواليه يسلم\n",
            "قعته والنديم والموت الملاعب عند الضمال دونه ولم ينرج\n",
            "منظرة نظراك الخرق الفاق الصهبيّ له الذي وسلم البطي\n",
            "لاحمد لي متلك يّاً بينك المشاق الصبحُ تصبحني سرُّ المسكور والمأوفا\n",
            "وراءك في الظنَّ الذي كبيراءةِ اقتصى قدِّى حلىً وأوصاما\n",
            "قد ميلوب السّماحل ناطقاً كالوزرُ أحلاكم بالناظم شاهد\n",
            "فأختر هذا وعشقٌوا ركباً فكأسةٍ بها لا تعلي بها كريمة قد نطقا\n",
            "تقضى أسه بمدمع ولها صرت الرسال\n",
            "أرفعت لها وهي التحاجير المظلم كله ودينهم أنطاهم لاح له ا\n",
            "إذا من مجد ذكر صدق الوعيد ولا هذا إنهم اللّه العرشدا\n",
            "وله إيان عيب غدا يمين وخطبوا يعاون الحق فالبيض خير\n",
            "نجر أن نشرا او شروا بأشيها آل بين المصطفى فمدادح\n",
            "يقام جثت كأسه بوجنانهم اليمين عُبِدَ اللّه عن فتحهم مترفُ\n",
            "عُمِّلها سرتعي بدَت فيالي زلتَ كله في النكن\n",
            "يُسميه أَعرفُ\n"
          ]
        }
      ],
      "source": [
        "# new try\n",
        "def generate(self, idx, max_new_tokens):\n",
        "    # idx is (B, T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "        # crop idx to the last block_size tokens\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        # get the predictions\n",
        "        logits, loss = self(idx_cond)\n",
        "        # focus only on the last time step\n",
        "        logits = logits[:, -1, :] # becomes (B, C)\n",
        "        # apply softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "        # sample from the distribution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "        # append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx\n",
        "model_new = torch.load(\"/content/drive/MyDrive/Datasets/poems_model.pth\")\n",
        "mn = model.to(device)\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(mn.generate(context, max_new_tokens=3000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}